{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Basic Natural Language Processing.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Natural Language Processing (NLP) became an important part of modern systems. it is used commonly in search engines, conversational interfaces, document processors and so on. Machines can handle structured data well but wen it comes to working with free form text, they will have a hard time. The purpose of NLP is to develop algorithms that enable computers to understand free - form text and help them understand language."
      ],
      "metadata": {
        "id": "n4O93z1UDW2D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4SMLCE3C6u6",
        "outputId": "477f6f40-a09a-4866-f09b-e0e7d8b60101"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: NLTK in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from NLTK) (2022.6.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from NLTK) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from NLTK) (4.64.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from NLTK) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "# install library\n",
        "! pip install NLTK"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#NLTK is the natrual language toolkit which help us to extract meaningful information from a given text data\n",
        "import nltk\n",
        "nltk.download()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvWum9OvEEUH",
        "outputId": "88cc1f4c-5818-4192-de9c-a10c36af57d0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to use the package called gensim and it is a robust semantic modeling library that is useful for many applications"
      ],
      "metadata": {
        "id": "G29w4ojTErWb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96HZPE4rEo13",
        "outputId": "d44935ce-8758-4e2d-b991-da3f93f70c55"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.7.3)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.6)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install pattern"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YR_tzmcE5Cd",
        "outputId": "1aaca3a8-65e9-466f-c8c7-bac4c2cb4246"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pattern\n",
            "  Downloading Pattern-3.6.0.tar.gz (22.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.2 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pattern) (0.16.0)\n",
            "Collecting backports.csv\n",
            "  Downloading backports.csv-1.0.7-py2.py3-none-any.whl (12 kB)\n",
            "Collecting mysqlclient\n",
            "  Downloading mysqlclient-2.1.1.tar.gz (88 kB)\n",
            "\u001b[K     |████████████████████████████████| 88 kB 4.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from pattern) (4.6.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from pattern) (4.2.6)\n",
            "Collecting feedparser\n",
            "  Downloading feedparser-6.0.10-py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 10.5 MB/s \n",
            "\u001b[?25hCollecting pdfminer.six\n",
            "  Downloading pdfminer.six-20220524-py3-none-any.whl (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 12.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pattern) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pattern) (1.7.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from pattern) (3.7)\n",
            "Collecting python-docx\n",
            "  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 38.7 MB/s \n",
            "\u001b[?25hCollecting cherrypy\n",
            "  Downloading CherryPy-18.7.0-py2.py3-none-any.whl (348 kB)\n",
            "\u001b[K     |████████████████████████████████| 348 kB 51.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pattern) (2.23.0)\n",
            "Collecting cheroot>=8.2.1\n",
            "  Downloading cheroot-8.6.0-py2.py3-none-any.whl (104 kB)\n",
            "\u001b[K     |████████████████████████████████| 104 kB 49.2 MB/s \n",
            "\u001b[?25hCollecting portend>=2.1.1\n",
            "  Downloading portend-3.1.0-py3-none-any.whl (5.3 kB)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from cherrypy->pattern) (8.13.0)\n",
            "Collecting jaraco.collections\n",
            "  Downloading jaraco.collections-3.5.2-py3-none-any.whl (10 kB)\n",
            "Collecting zc.lockfile\n",
            "  Downloading zc.lockfile-2.0-py2.py3-none-any.whl (9.7 kB)\n",
            "Collecting jaraco.functools\n",
            "  Downloading jaraco.functools-3.5.1-py3-none-any.whl (7.3 kB)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from cheroot>=8.2.1->cherrypy->pattern) (1.15.0)\n",
            "Collecting tempora>=1.8\n",
            "  Downloading tempora-5.0.2-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from tempora>=1.8->portend>=2.1.1->cherrypy->pattern) (2022.1)\n",
            "Collecting sgmllib3k\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "Collecting jaraco.text\n",
            "  Downloading jaraco.text-3.8.1-py3-none-any.whl (10 kB)\n",
            "Collecting jaraco.classes\n",
            "  Downloading jaraco.classes-3.2.2-py3-none-any.whl (6.0 kB)\n",
            "Collecting jaraco.context>=4.1\n",
            "  Downloading jaraco.context-4.1.2-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from jaraco.text->jaraco.collections->cherrypy->pattern) (5.8.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->jaraco.text->jaraco.collections->cherrypy->pattern) (3.8.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->pattern) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->pattern) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk->pattern) (4.64.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk->pattern) (2022.6.2)\n",
            "Collecting cryptography>=36.0.0\n",
            "  Downloading cryptography-37.0.4-cp36-abi3-manylinux_2_24_x86_64.whl (4.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.1 MB 46.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from pdfminer.six->pattern) (2.1.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=36.0.0->pdfminer.six->pattern) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->pattern) (2.21)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pattern) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pattern) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pattern) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pattern) (1.24.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from zc.lockfile->cherrypy->pattern) (57.4.0)\n",
            "Building wheels for collected packages: pattern, mysqlclient, python-docx, sgmllib3k\n",
            "  Building wheel for pattern (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pattern: filename=Pattern-3.6-py3-none-any.whl size=22332721 sha256=d41f6b54861ecbdeec0561e0e621123717c22a8e1182e989835768cfb316f55a\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/1f/4e/9b67afd2430d55dee90bd57618dd7d899f1323e5852c465682\n",
            "  Building wheel for mysqlclient (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mysqlclient: filename=mysqlclient-2.1.1-cp37-cp37m-linux_x86_64.whl size=99981 sha256=040d82c0b9ff7fc86e3c7c3a862fd04cd940e5250a208c765ae77cff4d95bf68\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/2d/67/2cb3f82e435fc8e055cb2761a15a0812bf086068f6fb835462\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184507 sha256=653ab2ecbc66784d8a3bbf3fd514bac72bd290c9b483b5447d514818a57c0f3e\n",
            "  Stored in directory: /root/.cache/pip/wheels/f6/6f/b9/d798122a8b55b74ad30b5f52b01482169b445fbb84a11797a6\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6066 sha256=8c1435187e364138ed4a7f727369546df37d33d969ac0def017050f294ffa7bb\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/ad/a4/0dff4a6ef231fc0dfa12ffbac2a36cebfdddfe059f50e019aa\n",
            "Successfully built pattern mysqlclient python-docx sgmllib3k\n",
            "Installing collected packages: jaraco.functools, jaraco.context, tempora, jaraco.text, jaraco.classes, zc.lockfile, sgmllib3k, portend, jaraco.collections, cryptography, cheroot, python-docx, pdfminer.six, mysqlclient, feedparser, cherrypy, backports.csv, pattern\n",
            "Successfully installed backports.csv-1.0.7 cheroot-8.6.0 cherrypy-18.7.0 cryptography-37.0.4 feedparser-6.0.10 jaraco.classes-3.2.2 jaraco.collections-3.5.2 jaraco.context-4.1.2 jaraco.functools-3.5.1 jaraco.text-3.8.1 mysqlclient-2.1.1 pattern-3.6 pdfminer.six-20220524 portend-3.1.0 python-docx-0.8.11 sgmllib3k-1.0.0 tempora-5.0.2 zc.lockfile-2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenizing text data**\n",
        "\n",
        "When we deal with text, we ned to break it down into smaller pieces for analysis. To do this tokenization can be applied. Tokenization is the process of dividing text into a set of pieces such as words or sentences. These pieces are called tokens. Depending on the applications or projects, our methods can divide text into many tokens."
      ],
      "metadata": {
        "id": "Szp5xoQLE-Qw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize, \\\n",
        "                word_tokenize, WordPunctTokenizer"
      ],
      "metadata": {
        "id": "j24usQf1Ffgq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define  the input text\n",
        "input_text = \"Do you know how tokenization works? It's actually quite interesting! Let's analyze a couple of sentences and figure it out.\" "
      ],
      "metadata": {
        "id": "q57j4mlXFsO5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "# Let's divide the input text into sentence tokens\n",
        "print(\"\\n Sentence tokenizer: \", sent_tokenize(input_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVsUzU8fGViI",
        "outputId": "16a356ed-3ad9-4415-b777-2dd39fc1a1eb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Sentence tokenizer:  ['Do you know how tokenization works?', \"It's actually quite interesting!\", \"Let's analyze a couple of sentences and figure it out.\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As can be seen from the above result, we did split the input_text int each sentence tokens"
      ],
      "metadata": {
        "id": "SnHrls0rGryM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's divide the input text into word tokens\n",
        "print(\"\\n Word tokenizer: \", word_tokenize(input_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "me8qxf77Gvv7",
        "outputId": "58dafb60-b026-4cc5-fc46-a44db6246c42"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Word tokenizer:  ['Do', 'you', 'know', 'how', 'tokenization', 'works', '?', 'It', \"'s\", 'actually', 'quite', 'interesting', '!', 'Let', \"'s\", 'analyze', 'a', 'couple', 'of', 'sentences', 'and', 'figure', 'it', 'out', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As can be seen from the above result, the input sentence is splitted into each word tokens."
      ],
      "metadata": {
        "id": "2O-LjuNpHAgs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# It is a time to divide the input text into word tokens using WordPunct\n",
        "print(\"\\n Word punct tokenizer: \", WordPunctTokenizer().tokenize(input_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdrXDl2kHHXR",
        "outputId": "152d8e31-862f-409b-d7c7-022b3116bb30"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Word punct tokenizer:  ['Do', 'you', 'know', 'how', 'tokenization', 'works', '?', 'It', \"'\", 's', 'actually', 'quite', 'interesting', '!', 'Let', \"'\", 's', 'analyze', 'a', 'couple', 'of', 'sentences', 'and', 'figure', 'it', 'out', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As can be seen from the above result, the input sentence is divided by each word tokens. The word 'its' in the above result, is divided by the punct tokenizer."
      ],
      "metadata": {
        "id": "45yrIhUsHXv1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Stemming and Lemmatization\n",
        "\n",
        "Working with text means wirking with a lot of variation. We must deal with various forms of the smae word and enable the computer to understand that these various words have the same base form. For example, the word sing can appear in amny forms, such as signer, singing, song, sung and so on. This set of words share similar meanings. This process is known as stemming. Stemming is a way of producing morpholotgical variants of a root/base word. Humans can easily identify these base forms and derive context.\n",
        "\n",
        "When analyzing text, it is useful to extract these base forms. Doing so enables the extraction of useful statistics derived from the input text. Stemming is one way to achive this. The goal of a stemmer is to reduce words from their various forms into a common base form. It is basially a heuristic process that cuts of the ends of words to extract their base forms."
      ],
      "metadata": {
        "id": "I7M1-AfZH2bd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing libraries\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "from nltk.stem.snowball import SnowballStemmer"
      ],
      "metadata": {
        "id": "HPi4l0LkH1si"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define some input words.\n",
        "input_words = ['writing', 'calves', 'be', 'branded', 'house', 'randomize', \n",
        "        'possibly', 'extraction', 'hospital', 'kept', 'scratchy', 'code']"
      ],
      "metadata": {
        "id": "xQo6ptFVKELi"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create objects for the Porter, Lancaster and snowball stemmers\n",
        "porter = PorterStemmer()\n",
        "lancaster = LancasterStemmer()\n",
        "snowball = SnowballStemmer('english')"
      ],
      "metadata": {
        "id": "XSqemjGfKRUv"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "it is a time to create a list of names for table display and format the output text."
      ],
      "metadata": {
        "id": "ZcSg3V4hKi29"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list of stemmer names for display\n",
        "stemmer_names = ['PORTER', 'LANCASTER', 'SNOWBALL']\n",
        "formatted_text = '{:>16}' * (len(stemmer_names) + 1)\n",
        "print('\\n', formatted_text.format('INPUT WORD', *stemmer_names),\n",
        "        '\\n', '='*68)\n",
        "# We need to Iterate through the words and stem them using the 3 stemmers\n",
        "for word in input_words:\n",
        "    output = [word, porter.stem(word), \n",
        "            lancaster.stem(word), snowball.stem(word)]\n",
        "    print(formatted_text.format(*output))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNjg9LSKKnNo",
        "outputId": "43f7a5ae-26c6-43ab-c696-73f785175da2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "       INPUT WORD          PORTER       LANCASTER        SNOWBALL \n",
            " ====================================================================\n",
            "         writing           write            writ           write\n",
            "          calves            calv            calv            calv\n",
            "              be              be              be              be\n",
            "         branded           brand           brand           brand\n",
            "           house            hous            hous            hous\n",
            "       randomize          random          random          random\n",
            "        possibly         possibl            poss         possibl\n",
            "      extraction         extract         extract         extract\n",
            "        hospital          hospit          hospit          hospit\n",
            "            kept            kept            kept            kept\n",
            "        scratchy        scratchi        scratchy        scratchi\n",
            "            code            code             cod            code\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As can be seen from the above result we did perform 3 stemming algorithms and all of them basically try to achive the same purpose. The only difference is the level of structness which is used to arrive at the base form.\n",
        "\n",
        "The Porter stemmer is the least struc and Lancaster is the strictest. Stemmers behave variously when it comes ot words such as possibly. The stemmed outputs obtained from the Lancaster stemmer are a bit of obfuscated because it reduce the words a lot. At the same time the algorithm is fast. The best algorithm is the Snowball stemmer because it is a good trade-off between speed and strictness."
      ],
      "metadata": {
        "id": "0IMGCXBULsak"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to lemmatization\n",
        "\n",
        "\n",
        "Lemmatization is another method of reducing words to their base forms. Lemmatization is the process of grouping together the different inflected forms of a word so they can be analyzed as a single item. Lemmatization is like stemming but it brings content to the words."
      ],
      "metadata": {
        "id": "Jplvq2BaMhaa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "gjFXauApM8PD"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_words = ['writing', 'calves', 'be', 'branded', 'house', 'randomize', \n",
        "        'possibly', 'extraction', 'hospital', 'kept', 'scratchy', 'code']"
      ],
      "metadata": {
        "id": "odE7lSXZNIre"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create lemmatizer object\n",
        "lemmatizer= WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "g4ma8tJUNLN1"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is a time to create a list of lemmatizer name for display"
      ],
      "metadata": {
        "id": "KQcAXkWVNUYi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "lemmatizer_names = ['NOUN LEMMATIZER', 'VERB LEMMATIZER']\n",
        "formatted_text = '{:>24}' * (len(lemmatizer_names) + 1)\n",
        "print('\\n', formatted_text.format('INPUT WORD', *lemmatizer_names), \n",
        "        '\\n', '='*75)\n",
        "for word in input_words:\n",
        "    output = [word, lemmatizer.lemmatize(word, pos='n'),\n",
        "           lemmatizer.lemmatize(word, pos='v')]\n",
        "    print(formatted_text.format(*output))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTjGkmCzNfAp",
        "outputId": "ecb05caa-09de-4473-f276-8266ea2fa533"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "               INPUT WORD         NOUN LEMMATIZER         VERB LEMMATIZER \n",
            " ===========================================================================\n",
            "                 writing                 writing                   write\n",
            "                  calves                    calf                   calve\n",
            "                      be                      be                      be\n",
            "                 branded                 branded                   brand\n",
            "                   house                   house                   house\n",
            "               randomize               randomize               randomize\n",
            "                possibly                possibly                possibly\n",
            "              extraction              extraction              extraction\n",
            "                hospital                hospital                hospital\n",
            "                    kept                    kept                    keep\n",
            "                scratchy                scratchy                scratchy\n",
            "                    code                    code                    code\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As can be seen from the above result, it works different than the stemming. If we see the word writing and calves it is very different from Stemming. The lemmatizer outputs are all meaningful, while the stemmer ouputs can be sometimes not meaningful."
      ],
      "metadata": {
        "id": "2SKNIov2N3fq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chunking\n",
        "\n",
        "Text data usually needs to be divided into pieces for further analysis. This process is known as chunking. This is used frequently in text analysis"
      ],
      "metadata": {
        "id": "9Mtx_SUyO8us"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from nltk.corpus import brown"
      ],
      "metadata": {
        "id": "zNyz9x6lO8K1"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is a need to define a function to divide the input text into chunks. The first parameter will be text and the second parameter will be the number of words in each chunk"
      ],
      "metadata": {
        "id": "05RS--RpPYxV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the input text into chunks, where each chunk contains N words\n",
        "def chunker(input_data, N):\n",
        "    input_words = input_data.split(' ')\n",
        "    output = []\n",
        "\n",
        "    cur_chunk = []\n",
        "    count = 0\n",
        "    for word in input_words:\n",
        "        cur_chunk.append(word)\n",
        "        count += 1\n",
        "        if count == N:\n",
        "            output.append(' '.join(cur_chunk))\n",
        "            count, cur_chunk = 0, []\n",
        "\n",
        "    output.append(' '.join(cur_chunk))\n",
        "\n",
        "    return output "
      ],
      "metadata": {
        "id": "vD5h59rJPlD1"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('brown')\n",
        "# Read the first 14000 woreds from the brown corpus\n",
        "input_data = ' '.join(brown.words()[:14000])\n",
        "# Define the number of words in each chunk\n",
        "chunk_size = 700\n",
        "# Divide input text into chunks and display the output\n",
        "chunks = chunker(input_data, chunk_size)\n",
        "print('\\nNumber of text chunks =', len(chunks), '\\n')\n",
        "for i, chunk in enumerate(chunks):\n",
        "    print('Chunk', i+1, '==>', chunk[:50])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPSgiPhGQik6",
        "outputId": "d1bc7c3d-302d-4d90-ccc9-341e049ea12e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of text chunks = 21 \n",
            "\n",
            "Chunk 1 ==> The Fulton County Grand Jury said Friday an invest\n",
            "Chunk 2 ==> '' . ( 2 ) Fulton legislators `` work with city of\n",
            "Chunk 3 ==> . Construction bonds Meanwhile , it was learned th\n",
            "Chunk 4 ==> , anonymous midnight phone calls and veiled threat\n",
            "Chunk 5 ==> Harris , Bexar , Tarrant and El Paso would be $451\n",
            "Chunk 6 ==> set it for public hearing on Feb. 22 . The proposa\n",
            "Chunk 7 ==> College . He has served as a border patrolman and \n",
            "Chunk 8 ==> of his staff were doing on the address involved co\n",
            "Chunk 9 ==> plan alone would boost the base to $5,000 a year a\n",
            "Chunk 10 ==> nursing homes In the area of `` community health s\n",
            "Chunk 11 ==> of its Angola policy prove harsh , there has been \n",
            "Chunk 12 ==> system which will prevent Laos from being used as \n",
            "Chunk 13 ==> reform in recipient nations . In Laos , the admini\n",
            "Chunk 14 ==> . He is not interested in being named a full-time \n",
            "Chunk 15 ==> said , `` to obtain the views of the general publi\n",
            "Chunk 16 ==> '' . Mr. Reama , far from really being retired , i\n",
            "Chunk 17 ==> making enforcement of minor offenses more effectiv\n",
            "Chunk 18 ==> to tell the people where he stands on the tax issu\n",
            "Chunk 19 ==> '' . Trenton -- William J. Seidel , state fire war\n",
            "Chunk 20 ==> not comment on tax reforms or other issues in whic\n",
            "Chunk 21 ==> \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to bag of word\n",
        "\n",
        "One of the main purpose of text  analysis with bag of words model is to convert text into numerical form so that we can use machine learning on it. For instance, there is a text documents which contain many millions of words. To analyze these documents, we need to extract the text and convert it into a form of numerical representation.\n",
        "\n",
        "Machine learning algorithms need numerical data to work with so that they can analyze the data and extract meaningful information. This is where bag of words model comes in. This model extracts vocabulary from all the words in the documents and build a model using a document-term matrix. This allows us to represent every document as a bag of words. We just keep track of word counts and disregard the grammatical details and the word order.\n",
        "\n",
        "A document-term matrix is basically a table gives us counts of various words that occur in a document. So a text document can be represented as a weighted combination of various words. We can se thresholds and choose words that are more meaningful. In a way we are building a histogram of all the words in the document that will be used as a feature vector and this feature vector is used for text classification."
      ],
      "metadata": {
        "id": "a1EqyP_Av12R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n"
      ],
      "metadata": {
        "id": "71WIuKw1xQpE"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_data = ' '.join(brown.words()[:5500])\n",
        "chunk_size = 800\n",
        "text = chunker(input_data, chunk_size)"
      ],
      "metadata": {
        "id": "VcIZ9sLax5sW"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to dict items\n",
        "chunks = []\n",
        "for count, chunk in enumerate(text):\n",
        "    d = {'index': count, 'text': chunk}\n",
        "    chunks.append(d)"
      ],
      "metadata": {
        "id": "KmtutcP6yJiQ"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we will extract the document term matrix where we get the count of each word. We can do this by using CountVectorizer method, which will take 2 input parameters. The first parameter is the minimum document frequency and the second parameter is the maximum document frequency. The frequency means the number of occurrences of a word in a text."
      ],
      "metadata": {
        "id": "mYD1adJayiDX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the document term matrix\n",
        "count_vectorizer = CountVectorizer(min_df=7, max_df=20)\n",
        "document_term_matrix = count_vectorizer.fit_transform([chunk['text'] for chunk in chunks])"
      ],
      "metadata": {
        "id": "LmOIhyzrygQq"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we will extract the vocabulary with the Bag of words model and display it. The vocabulary is defined as the list of distinct words that were extracted."
      ],
      "metadata": {
        "id": "zcIm-ljMzSFy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = np.array(count_vectorizer.get_feature_names())\n",
        "print(\"\\nVocabulary:\\n\", vocabulary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSRqRA1ZzQY5",
        "outputId": "01913368-97b6-40fd-91eb-fa79889951c9"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Vocabulary:\n",
            " ['and' 'are' 'be' 'by' 'county' 'for' 'in' 'is' 'it' 'of' 'on' 'one'\n",
            " 'said' 'state' 'that' 'the' 'to' 'two' 'was' 'which' 'with']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate names for chunks\n",
        "chunk_names = []\n",
        "for i in range(len(text)):\n",
        "    chunk_names.append('Chunk-' + str(i+1))"
      ],
      "metadata": {
        "id": "8BBHcJtlzt04"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print document term matrix\n",
        "print(\"\\nDocument term matrix:\")\n",
        "formatted_text = '{:>12}' * (len(chunk_names) + 1)\n",
        "print('\\n', formatted_text.format('Word', *chunk_names), '\\n')\n",
        "for word, item in zip(vocabulary, document_term_matrix.T):\n",
        "    # 'item' is a 'csr_matrix' data structure\n",
        "    output = [word] + [str(freq) for freq in item.data]\n",
        "    print(formatted_text.format(*output))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdKXbMIGz9V3",
        "outputId": "83cc41e7-047f-49d6-c747-c157018fde33"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Document term matrix:\n",
            "\n",
            "         Word     Chunk-1     Chunk-2     Chunk-3     Chunk-4     Chunk-5     Chunk-6     Chunk-7 \n",
            "\n",
            "         and          23           9           9          11           9          17          15\n",
            "         are           2           2           1           1           2           2           2\n",
            "          be           6           8           7           7           6           2           2\n",
            "          by           3           4           4           5          14           3           7\n",
            "      county           6           2           7           3           1           2           2\n",
            "         for           7          13           4          10           7           6           5\n",
            "          in          15          11          15          11          13          14          19\n",
            "          is           2           7           3           4           5           5           2\n",
            "          it           8           6           8           9           3           1           2\n",
            "          of          31          20          20          30          29          35          27\n",
            "          on           4           3           5          10           6           5           3\n",
            "         one           1           3           1           2           2           1           1\n",
            "        said          12           5           7           7           4           3           7\n",
            "       state           3           7           2           6           3           4           1\n",
            "        that          13           8           9           2           7           1           7\n",
            "         the          71          51          43          51          43          52          51\n",
            "          to          11          26          20          26          21          15          14\n",
            "         two           2           1           1           1           1           2           2\n",
            "         was           5           6           7           7           4           7           3\n",
            "       which           7           4           5           4           3           1           1\n",
            "        with           2           2           3           1           2           2           4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As can be seen from the above in the bag of words model document-term matrix along with the corresponding counts in each chunk."
      ],
      "metadata": {
        "id": "UjBbwaF60xkM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "p6VCOgbX09NE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}